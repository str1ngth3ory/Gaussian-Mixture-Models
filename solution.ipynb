{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Automatic image processing is a key component to many AI systems, including facial recognition and video compression, instance segmentation of images and point cloud data. One basic method for processing is segmentation, by which we divide an image into a fixed number of components in order to simplify its representation. For example, we can train a mixture of Gaussians to represent an image, and segment it according to the simplified representation as shown in the images below.\n",
    "\n",
    "![alt text](images/k6_bird_color_24.png)\n",
    "\n",
    "Or we could perform a clustering of point cloud in order to separate different objects, backgrounds etc, as shown in the image below\n",
    "\n",
    "![alt text](images/pcd_clustered.gif)\n",
    "\n",
    "In this assignment, you will learn to perform image compression and point cloud segmentation. To this end, you will implement Gaussian mixture models and iteratively improve their performance. First you will perform segmentation on the \"Bird\" (`bird_color_24.png`) and at the end run your algorithm on 3D point cloud data.\n",
    "\n",
    "To begin, you will implement several methods of image segmentation, with increasing complexity:\n",
    "\n",
    "1. Implement k-means clustering to segment a color image.\n",
    "\n",
    "2. Familiarize yourself with the algorithm by running it on simple dataset.\n",
    "\n",
    "3. Build a Gaussian mixture model to be trained with expectation-maximization.\n",
    "\n",
    "4. Experiment with varying the details of the Gaussian mixture modelâ€™s implementation.\n",
    "\n",
    "5. Implement and test a new metric called the Bayesian information criterion, which guarantees a more robust image segmentation.\n",
    "\n",
    "<br>\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Note on Vectorization\n",
    "\n",
    "The concept of Vectorization was introduced in the last section of Assignment 4. For this assignment, please vectorize your code wherever possible using numpy arrays, instead of running for-loops over the images being processed.\n",
    "\n",
    "For example of how this might be useful, consider the following array:\n",
    "A = [12 34 1234 764 ...(has a million values)... 91, 78]\n",
    "\n",
    "Now you need to calculate another array B, which has the same dimensions as A above. Say each value in B is calculated as follows:\n",
    "(each value in B) = square_root_of(some constants pi log(k) * (each value in A))/7\n",
    "\n",
    "You might wish to use a for-loop to compute this. However, it will take really long to run on an array of this magnitude.\n",
    "Alternatively, you may choose to use numpy and perform this calculation in a single line. You can pass A as a numpy array and the entire calculation will be done in a line, resulting in B being populated with the corresponding values that come out of this formula.\n",
    "\n",
    "Check out **Basic Operation** section of the Numpy Tutorial if you are not familiar with numpy vector/matrix operations: https://docs.scipy.org/doc/numpy-1.15.1/user/quickstart.html#basic-operations\n",
    "\n",
    "#### Please note that numpy.vectorize DOES NOT perform vectorization, it only does a loop. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: K-means Clustering (19 pts)\n",
    "\n",
    "One easy method for image segmentation is to simply cluster all similar data points together and then replace their values with the mean value. Thus, we'll warm up using k-means clustering. This will also provide a baseline to compare with your segmentation. Please note that clustering will come in handy later.\n",
    "\n",
    "Fill out `get_initial_means()`, `k_means_step()` functions below.\n",
    "\n",
    "In `get_initial_means()`, you should choose  k random points from the data (without replacement) to use as initial cluster means.\n",
    "\n",
    "Your code will be unit tested automatically when you run the cell (`Cell > Run Cells OR Shift + Enter`).\n",
    "\n",
    "#### Try to vectorize the code for it to run faster. Without vectorization it takes 25-30 minutes for the code to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Run this cell and check if you have all necessary modules\n",
    "from IPython.html.widgets import *\n",
    "import mixture_tests as tests\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from helper_functions import *\n",
    "# Please don't modify this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# DON'T RUN THIS CELL IT WILL THROW AN ERROR\n",
    "# IF YOU ACCIDENTALL RUN IT, IT'S OK, YOU CAN IGNORE THE ERRORS\n",
    "import numpy as np\n",
    "from .helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# DON'T RUN THIS CELL IT WILL THROW AN ERROR\n",
    "# IF YOU ACCIDENTALL RUN IT, IT'S OK, YOU CAN IGNORE THE ERRORS\n",
    "import numpy as np\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_initial_means(array, k):\n",
    "    \"\"\"\n",
    "    Picks k random points from the 2D array \n",
    "    (without replacement) to use as initial \n",
    "    cluster means\n",
    "\n",
    "    params:\n",
    "    array = numpy.ndarray[numpy.ndarray[float]] - m x n | datapoints x features\n",
    "\n",
    "    k = int\n",
    "\n",
    "    returns:\n",
    "    initial_means = numpy.ndarray[numpy.ndarray[float]]\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.K_means_test().test_initial_means(get_initial_means)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def k_means_step(X, k, means):\n",
    "    \"\"\"\n",
    "    A single update/step of the K-means algorithm\n",
    "    Based on a input X and current mean estimate\n",
    "    calculate new means and predict clusters for each of the pixel\n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n | pixels x features (already flattened)\n",
    "    k = int\n",
    "    means = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "\n",
    "    returns:\n",
    "    (new_means, clusters)\n",
    "    new_means = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    clusters = numpy.ndarray[int] - m sized vector\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.K_means_test().test_k_means_step(k_means_step)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means - Visualizing the results\n",
    "\n",
    "Now that you are done with the K-means step implementation lets try to visualize what's happening if you repeat these steps multiple times.\n",
    "\n",
    "**You don't need to be implementing anything in the next cells until Image Segmentation Section**, but you are highly encouraged to play with parameters and datasets, to get a sense of what is happening at every algorithm iteration step.\n",
    "\n",
    "Feel free to explore and improve the function below, it will be used for visualizing K-means progress\n",
    "but it's not required and WON'T effect your grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains a code for loading a dataset from the `data` folder\n",
    "# Each of these datasets contains synthtic (generated) data\n",
    "# You can simply run this cell for now and come back to it later if you want to make changes\n",
    "# Make sure you implemented everything in cells above and passed the unittests\n",
    "def K_means_2D_dataset(dataset_index, K):\n",
    "    # Load the dataset from data folder\n",
    "    X = np.loadtxt(\"data/%d_dataset_X.csv\" % dataset_index, delimiter=\",\")\n",
    "    print(\"The dataset is of a size:\", X.shape)\n",
    "\n",
    "    # Load the labels\n",
    "    # Clustering is unsupervised method, where no labels are provided\n",
    "    # However, since we generated the data outselves we know the clusters,\n",
    "    # and load them for illustration purposes.\n",
    "    y = np.int16(np.loadtxt(\"data/%d_dataset_y.csv\" % dataset_index, delimiter=\",\"))\n",
    "\n",
    "    # Feel free to edit the termination condition for the K-means algorithm\n",
    "    # Currently is just runs for n_iterations, before terminating\n",
    "    n_iterations = 10\n",
    "    m,n = X.shape\n",
    "    means = get_initial_means(X,K)\n",
    "    clusters = np.zeros([n])\n",
    "    # keeping track of how clusters and means changed, for visualization purposes\n",
    "    means_history = [means]\n",
    "    clusters_history = [clusters] \n",
    "    for iteration_i in range(n_iterations):\n",
    "        means, clusters = k_means_step(X, K, means)\n",
    "        clusters_history.append(clusters)\n",
    "\n",
    "    return X, y, means_history, clusters_history\n",
    "\n",
    "# Things to try:\n",
    "# - Try different initialization to see check initialization robustness\n",
    "# - Improve the termination condition (you will be able to reuse later as well!)\n",
    "# - Try creating you own dataset in the `data/` folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN - TRY DIFFERENT PARAMETERS - REPEAT\n",
    "dataset_index = 2 # for different dataset change it to number from [0,4]\n",
    "K = 5 # Number of clusters - play with this number\n",
    "\n",
    "X, y, means_history, clusters_history = K_means_2D_dataset(dataset_index, K)\n",
    "\n",
    "# This is an interactive cell to see the progress of training your K-means algorithm.\n",
    "# Feel free to improve the visualization code and share it with your classmates on Piazza\n",
    "def get_cluster(i):\n",
    "    clusters = clusters_history[i] # Get the clusters from K-means' i-th iteration\n",
    "    plt.figure(None, figsize=(15,6)) # Set the plot size\n",
    "    plt.suptitle('Drag the slider to see the algorthm training progress')\n",
    "    ax1=plt.subplot(1, 2, 1)\n",
    "    ax1.set_title('K-means clsuters - step %d' % i)\n",
    "    for k in range(K):\n",
    "        plt.plot(X[clusters==k,0], X[clusters==k,1], '.') # \n",
    "        # Try to plot the centers of the clusters \n",
    "        # You can access them by calling means_history[i]\n",
    "        # How could you plot the area that belong to that cluster?\n",
    "\n",
    "    # Just to get a flavour of how the data looks like\n",
    "    ax2=plt.subplot(1, 2, 2)\n",
    "    ax2.set_title('Ground truth clusters')\n",
    "    for i in np.unique(y):\n",
    "        ax2.plot(X[y==i,0],X[y==i,1],'.')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "interactive(get_cluster, i=(1,len(clusters_history)-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image segmentation (X Points)\n",
    "2D data clustering is all cool and all but now it's time to use K-means for the image compression! \n",
    "\n",
    "Fill in the `k_means_segment()` function below, you will find your `k_means_step()` and `get_initial_means()` very handy here. \n",
    "\n",
    "You will separate the provided RGB values into k clusters using the k-means algorithm, then return an updated version of the image with the original values replaced with the corresponding cluster center values.\n",
    "\n",
    "Your convergence test should be whether the assigned clusters stop changing. Note that this convergence test is rather slow. When no initial cluster means (`initial_means`) are provided, you need to initialize them yourself, based on the given k.\n",
    "\n",
    "For this part of the assignment, since clustering is best used on multidimensional data, we will be using the color image `bird_color_24.png`.\n",
    "\n",
    "Please pay close attention to the dimensions of the data. In the `k_means_step()` you were working with m x n data, here the input is an image (`image_values`) which has a shape of **rows x columns x color_channels**.\n",
    "\n",
    "The function should return an updated version of the image with the original values replaced with the corresponding cluster values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def k_means_segment(image_values, k=3, initial_means=None):\n",
    "    \"\"\"\n",
    "    Separate the provided RGB values into\n",
    "    k separate clusters using the k-means algorithm,\n",
    "    then return an updated version of the image\n",
    "    with the original values replaced with\n",
    "    the corresponding cluster values.\n",
    "\n",
    "    params:\n",
    "    image_values = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - r x c x ch\n",
    "    k = int\n",
    "    initial_means = numpy.ndarray[numpy.ndarray[float]] or None\n",
    "\n",
    "    returns:\n",
    "    updated_image_values = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - r x c x ch\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.K_means_test().test_k_means(k_means_segment)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visulizing K-means segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5 # number of clusters - feel free to play with it\n",
    "\n",
    "image_values = image_to_matrix('images/bird_color_24.png')\n",
    "# Play with the K value below to see the effect number of clusters have\n",
    "new_image = k_means_segment(image_values, k=k)\n",
    "\n",
    "plt.figure(None,figsize=(9,12))\n",
    "plt.imshow(new_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can reuse the K-means visualization code from previous section to show the training progress on the image for different iterations and even numbers of clusters.\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing a Multivariate Gaussian Mixture Model (40 pts)\n",
    "\n",
    "Next, we will step beyond clustering and implement a complete Gaussian mixture model.\n",
    "\n",
    "But, before you dive into the code, you are highly encouraged to go over `read/gaussians.pdf` file before you start, to familiarize yourself with multivariate case of the Gaussian distribution.\n",
    "\n",
    "In addition to that, there is a great ~17 minute where Alexander Ihler goes over nuts and bolds of the multivariate EM algorithm details on Youtube:\n",
    "https://www.youtube.com/watch?v=qMTuMa86NzU\n",
    "\n",
    "Another resource you can refer to is the `read/em.pdf` document attached, which is a chapter from Pattern Recognition and Machine Learning book by Christopher M. Bishop.\n",
    "\n",
    "- - - \n",
    "\n",
    "Now, it's time to complete the implementation of the functions below what will later assemble into a Multivariate Gaussian Expectation Maximization algorithm:\n",
    "\n",
    "1. Calculate the probability of a given data point (e.g. rgb value of a pixel) of belonging to a specific Gaussian component. (5 points)\n",
    "\n",
    "2. Use expectation-maximization (EM) to train the model to represent the image as a mixture of Gaussians. (20 points)\n",
    "\n",
    "To initialize EM, set each component's mean to the means value of randomly chosen pixels (same as for K-means) and calculate covariances based on the selected means, and set the mixing coefficients to a uniform distribution. \n",
    "\n",
    "We've set the convergence condition for you in `default_convergence()` (see `helper_functions.py` file): if the new likelihood is within 10% of the previous likelihood for 10 consecutive iterations, the model has converged.\n",
    "\n",
    "**Note:** there are packages that can run EM automagically, but you have to implement your own version of EM without using these extra packages. **It also means that you are not allowed to look into any implementations of the algorithms, e.g scikit-learn and many others. NumPy is your only tool here.** \n",
    "\n",
    "3. Calculate the log likelihood of the trained model. (5 points)\n",
    "4. Segment the image according to the trained model. (5 points)\n",
    "5. Determine the best segmentation by iterating over model training and scoring, since EM isn't guaranteed to converge to the global maximum. (5 points)\n",
    "\n",
    "It'd be helpful to implement the above functions in the following order - \n",
    "1. initialize_parameters\n",
    "2. prob\n",
    "3. E_step\n",
    "4. M_step\n",
    "5. likelihood \n",
    "6. train_model\n",
    "7. cluster\n",
    "8. segment\n",
    "9. best_segment\n",
    "\n",
    "We've provided comments in `mixture_models.py` to help you all along. We have also provided the necessary tests for this part in `mixture_tests.py`. Please make sure to use these tests before submitting to Bonnie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning: You may lose all marks for this part if your code runs for too long.\n",
    "\n",
    "### You will need to vectorize your code in this part. Specifically, the method E_step() and M_step() which make up the train_model(), perform operations using numpy arrays. These are time-sensitive functions and will be called over and over as you proceed with this assignment.\n",
    "\n",
    "For the synthetic data test which we provide to check if your training is working, the set is too small and it won't make a difference. But with the actual image that we use ahead, for-loops won't do good. Vectorized code would take under 30 seconds to converge which would typically involve about 15-20 iterations with the convergence function we have here. Inefficient code that uses loops or iterates over each pixel value sequentially, will take hours to run. You don't want to do that.\n",
    "\n",
    "- - -\n",
    "\n",
    "Same as in K-means you will be working with the data of size (m x n). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def initialize_parameters(X, k):\n",
    "    \"\"\"\n",
    "    Return initial values for training of the GMM\n",
    "    Set component mean to a random\n",
    "    pixel's value (without replacement),\n",
    "    based on the mean calculate covariance matrices,\n",
    "    and set each component mixing coefficient (PIs)\n",
    "    to a uniform values\n",
    "    (e.g. 4 components -> [0.25,0.25,0.25,0.25]).\n",
    "    \n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    k = int\n",
    "    \n",
    "    returns:\n",
    "    (MU, SIGMA, PI)\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    PI = numpy.ndarray[float] - k x 1 \n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_initialization(initialize_parameters)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def prob(x, mu, sigma):\n",
    "    \"\"\"Calculate the probability of a single\n",
    "    data point x under component with\n",
    "    the given mean and covariance.\n",
    "    # NOTE: there is nothing to vectorize here yet,\n",
    "    # it's a simple check to make sure you got the\n",
    "    # multivariate normal distribution formula right\n",
    "    # which is given by N(x;MU,SIGMA) above\n",
    "\n",
    "    params:\n",
    "    x = numpy.ndarray[float]\n",
    "    mu = numpy.ndarray[float]\n",
    "    sigma = numpy.ndarray[numpy.ndarray[float]]\n",
    "\n",
    "    returns:\n",
    "    probability = float\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_prob(prob)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def E_step(X,MU,SIGMA,PI,k):\n",
    "    \"\"\"\n",
    "    E-step - Expectation \n",
    "    Calculate responsibility for each\n",
    "    of the data points, for the given \n",
    "    MU, SIGMA and PI.\n",
    "    \n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    PI = numpy.ndarray[float] - k x 1\n",
    "    k = int\n",
    "    \n",
    "    returns:\n",
    "    responsibility = numpy.ndarray[numpy.ndarray[float]] - k x m\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_e_step(E_step)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def M_step(X, r, k):\n",
    "    \"\"\"\n",
    "    M-step - Maximization\n",
    "    Calculate new MU, SIGMA and PI matrices\n",
    "    based on the given responsibilities.\n",
    "    \n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    r = numpy.ndarray[numpy.ndarray[float]] - k x m\n",
    "    k = int\n",
    "    \n",
    "    returns:\n",
    "    (new_MU, new_SIGMA, new_PI)\n",
    "    new_MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    new_SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    new_PI = numpy.ndarray[float] - k x 1\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_m_step(M_step)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def likelihood(X, PI, MU, SIGMA, k):\n",
    "    \"\"\"Calculate a log likelihood of the \n",
    "    trained model based on the following\n",
    "    formula for posterior probability:\n",
    "    \n",
    "    log10(Pr(X | mixing, mean, stdev)) = sum((n=1 to N), log10(sum((k=1 to K),\n",
    "                                      mixing_k * N(x_n | mean_k,stdev_k))))\n",
    "\n",
    "    Make sure you are using log base 10, instead of log base 2.\n",
    "    \n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    PI = numpy.ndarray[float] - k x 1\n",
    "    k = int\n",
    "\n",
    "    returns:\n",
    "    log_likelihood = float\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_likelihood(likelihood)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def train_model(X, k, convergence_function, initial_values = None):\n",
    "    \"\"\"\n",
    "    Train the mixture model using the \n",
    "    expectation-maximization algorithm. \n",
    "    E.g., iterate E and M steps from \n",
    "    above until convergence.\n",
    "    If the initial_values are None, initialize them.\n",
    "    Else it's a tuple of the format (MU, SIGMA, PI).\n",
    "    Convergence is reached when convergence_function\n",
    "    returns terminate as True,\n",
    "    see default convergence_function example \n",
    "    in `helper_functions.py`\n",
    "\n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    k = int\n",
    "    convergence_function = func\n",
    "    initial_values = None or (MU, SIGMA, PI)\n",
    "\n",
    "    returns:\n",
    "    (new_MU, new_SIGMA, new_PI, responsibility)\n",
    "    new_MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    new_SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    new_PI = numpy.ndarray[float] - k x 1\n",
    "    responsibility = numpy.ndarray[numpy.ndarray[float]] - k x m\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_train(train_model, likelihood)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def cluster(r):\n",
    "    \"\"\"\n",
    "    Based on a given responsibilities matrix\n",
    "    return an array of cluster indices.\n",
    "    Assign each datapoint to a cluster based,\n",
    "    on component with a max-likelihood \n",
    "    (maximum responsibility value).\n",
    "    \n",
    "    params:\n",
    "    r = numpy.ndarray[numpy.ndarray[float]] - k x m - responsibility matrix\n",
    "    \n",
    "    return:\n",
    "    clusters = numpy.ndarray[int] - m x 1 \n",
    "    \"\"\"\n",
    "    # TODO: finish this\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_cluster(cluster)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def segment(X, MU, k, r):\n",
    "    \"\"\"\n",
    "    Segment the X matrix into k components. \n",
    "    Returns a matrix where each data point is \n",
    "    replaced with its max-likelihood component mean.\n",
    "    E.g., return the original matrix where each pixel's\n",
    "    intensity replaced with its max-likelihood \n",
    "    component mean. (the shape is still mxn, not \n",
    "    original image size)\n",
    "\n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    k = int\n",
    "    r = numpy.ndarray[numpy.ndarray[float]] - k x m - responsibility matrix\n",
    "\n",
    "    returns:\n",
    "    new_X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_segment(train_model, segment)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def best_segment(X,k,iters):\n",
    "    \"\"\"Determine the best segmentation\n",
    "    of the image by repeatedly\n",
    "    training the model and\n",
    "    calculating its likelihood.\n",
    "    Return the segment with the\n",
    "    highest likelihood.\n",
    "\n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    k = int\n",
    "    iters = int\n",
    "\n",
    "    returns:\n",
    "    (likelihood, segment)\n",
    "    likelihood = float\n",
    "    segment = numpy.ndarray[numpy.ndarray[float]]\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_best_segment(best_segment)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GMM - Visualizing the results\n",
    "\n",
    "Now that you are done with the EM implementation lets try to visualize what's happening if you repeat these steps multiple times.\n",
    "\n",
    "**You don't need to be implementing anything in the next 2 cells, but you are highly encouraged to play with parameters and datasets, to get a visual sense of what is happening at every step.\n",
    "\n",
    "\n",
    "Feel free to explore and improve the function below, it will be used for visualizing GMM progress\n",
    "but it's not required and WON'T effect your grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMM_2D_dataset(dataset_index, K):\n",
    "    # Load the dataset from data folder\n",
    "    X = np.loadtxt(\"data/%d_dataset_X.csv\" % dataset_index, delimiter=\",\")\n",
    "    print(\"There are %d datapoints in the current dataset, each of a size %d\" % X.shape)\n",
    "    # Load the labels\n",
    "    # Clustering is unsupervised method, where no labels are provided\n",
    "    # However, since we generated the data outselves we know the labels,\n",
    "    # and load them for illustration purposes.\n",
    "    y = np.int16(np.loadtxt(\"data/%d_dataset_y.csv\" % dataset_index, delimiter=\",\"))\n",
    "    # Feel free to edit the termination condition for the EM algorithm\n",
    "    # Currently is just runs for n_iterations, before terminating\n",
    "    \n",
    "    MU, SIGMA, PI = initialize_parameters(X, K)\n",
    "    \n",
    "    clusters_history = []\n",
    "    for _ in range(200):\n",
    "        r = E_step(X,MU,SIGMA,PI,K)\n",
    "        new_MU, new_SIGMA, new_PI = M_step(X, r, K)\n",
    "        PI, MU, SIGMA = new_PI, new_MU, new_SIGMA\n",
    "        clusters = cluster(r)\n",
    "        clusters_history.append(clusters)\n",
    "\n",
    "    return X, y, clusters_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY DIFFERENT PARAMETERS\n",
    "dataset_index = 3 # for different dataset change it to number from [0,5]\n",
    "K = 3 # Number of clusters - play with this number\n",
    "\n",
    "X, y, clusters_history = GMM_2D_dataset(dataset_index, K)\n",
    "\n",
    "# This is an interactive cell to see the progress of training your GMM algorithm.\n",
    "# Feel free to improve the visualization code and share it with your classmates on Piazza.\n",
    "def get_cluster(i):\n",
    "    clusters = clusters_history[i] # Get the clusters from K-means' i-th iteration\n",
    "    plt.figure(None, figsize=(15,6)) # Set the plot size\n",
    "    plt.suptitle('Drag the slider to see the algorthm training progress')\n",
    "    ax1=plt.subplot(1, 2, 1)\n",
    "    ax1.set_title('K-means clsuters - step %d' % i)\n",
    "    for k in range(K):\n",
    "        plt.plot(X[clusters==k,0], X[clusters==k,1], '.') # \n",
    "        # Try to plot the centers of the clusters \n",
    "        # You can access them by calling means_history[i]\n",
    "        # How could you plot the area that belong to that cluster?\n",
    "\n",
    "    # Just to get a flavour of how the data looks like\n",
    "    ax2=plt.subplot(1, 2, 2)\n",
    "    ax2.set_title('Ground truth clusters')\n",
    "    for i in np.unique(y):\n",
    "        ax2.plot(X[y==i,0],X[y==i,1],'.')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "interactive(get_cluster, i=(0,len(clusters_history)-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the image compression results of GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = 'images/bird_color_24.png' # Image path\n",
    "original_image_matrix = image_to_matrix(image_file) # Save original image\n",
    "image_matrix = original_image_matrix.reshape(-1,3) # collapse the dimension\n",
    "K = 10 # K\n",
    "\n",
    "_, best_seg = best_segment(image_matrix, K, iters = 10)\n",
    "new_image = best_seg.reshape(*original_image_matrix.shape) # reshape collapsed matrix to original size\n",
    "# Show the image\n",
    "plt.figure(None,figsize=(9,12))\n",
    "plt.imshow(new_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model Experimentation (20 pts)\n",
    "\n",
    "We'll now experiment with a few methods for improving GMM performance.\n",
    "\n",
    "## Part 3a: Improved Initialization \n",
    "\n",
    "12.5 points\n",
    "\n",
    "To run EM in our baseline Gaussian mixture model, we use random initialization to determine the initial values for our component means. We can do better than this!\n",
    "\n",
    "Fill in `improved_initialization()` by training a GMM to find initial means. This type of initialization differs from simply increasing training time because we \"reset\" the covariance and mixing coefficient parameters. That is, for training, we recompute covariance parameters based on the means we learned during initialization and again set the mixing coefficients to a uniform distribution. A GMM tends to converge to elongated covariances, so by resetting these parameters we have a higher chance of avoiding local maxima.\n",
    "\n",
    "Please don't use any external packages for anything other than basic calculations. Note that your improvement might significantly slow down runtime, although we don't expect you to spend more than 10 minutes on initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def improved_initialization(X,k):\n",
    "    \"\"\"\n",
    "    Initialize the training\n",
    "    process by setting each\n",
    "    component mean using some algorithm that\n",
    "    you think might give better means to start with,\n",
    "    based on the mean calculate covariance matrices,\n",
    "    and set each component mixing coefficient (PIs)\n",
    "    to a uniform values\n",
    "    (e.g. 4 components -> [0.25,0.25,0.25,0.25]).\n",
    "    \n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    k = int\n",
    "    \n",
    "    returns:\n",
    "    (MU, SIGMA, PI)\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    PI = numpy.ndarray[float] - k x 1 \n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_improvement(improved_initialization, initialize_parameters, train_model, likelihood)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3b: Convergence Condition\n",
    "\n",
    "7.5 points\n",
    "\n",
    "You might be skeptical of the convergence criterion we've provided in `default_convergence()`. To test out another convergence condition, implement `new_convergence_condition()` to return true if all the new model parameters (means, variances, and mixing coefficients) are within 10% of the previous variables for 10 consecutive iterations. This will mean re-implementing `train_model()` in the `train_model_improved()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def new_convergence_function(previous_variables, new_variables, conv_ctr,\n",
    "                             conv_ctr_cap=10):\n",
    "    \"\"\"\n",
    "    Convergence function\n",
    "    based on parameters:\n",
    "    when all variables vary by\n",
    "    less than 10% from the previous\n",
    "    iteration's variables, increase\n",
    "    the convergence counter.\n",
    "\n",
    "    params:\n",
    "    previous_variables = [numpy.ndarray[float]]\n",
    "                         containing [means, variances, mixing_coefficients]\n",
    "    new_variables = [numpy.ndarray[float]]\n",
    "                    containing [means, variances, mixing_coefficients]\n",
    "    conv_ctr = int\n",
    "    conv_ctr_cap = int\n",
    "\n",
    "    return:\n",
    "    (conv_crt, converged)\n",
    "    conv_ctr = int\n",
    "    converged = boolean\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def train_model_improved(X, k, convergence_function, initial_values = None):\n",
    "    \"\"\"\n",
    "    Train the mixture model using the \n",
    "    expectation-maximization algorithm. \n",
    "    E.g., iterate E and M steps from \n",
    "    above until convergence.\n",
    "    If the initial_values are None, initialize them.\n",
    "    Else it's a tuple of the format (MU, SIGMA, PI).\n",
    "    Convergence is reached when convergence_function\n",
    "    returns terminate as True. Use new_convergence_fuction \n",
    "    implemented above. \n",
    "\n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    k = int\n",
    "    convergence_function = func\n",
    "    initial_values = None or (MU, SIGMA, PI)\n",
    "\n",
    "    returns:\n",
    "    (new_MU, new_SIGMA, new_PI, responsibility)\n",
    "    new_MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    new_SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    new_PI = numpy.ndarray[float] - k x 1\n",
    "    responsibility = numpy.ndarray[numpy.ndarray[float]] - k x m\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "# Unittest below will check both of the functions at the same time. \n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_convergence_condition(improved_initialization, train_model_improved, initialize_parameters, train_model, likelihood, new_convergence_function)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Bayesian Information Criterion (20 pts)\n",
    "\n",
    "In our previous solutions, our only criterion for choosing a model was whether it maximizes the posterior likelihood regardless of how many parameters this requires. As a result, the \"best\" model may simply be the model with the most parameters, which would be overfit to the training data.\n",
    "\n",
    "To avoid overfitting, we can use the [Bayesian information criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion) (a.k.a. BIC) which penalizes models based on the number of parameters they use. In the case of the Gaussian mixture model, this is equal to the number of components times the number of variables per component (mean, variance and mixing coefficient).\n",
    "\n",
    "## Part 4a: Implement BIC\n",
    "\n",
    "5 points\n",
    "\n",
    "Implement `bayes_info_criterion()` to calculate the BIC of a trained Gaussian Mixture Model (based on the given parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def bayes_info_criterion(X, PI, MU, SIGMA, k):\n",
    "    \"\"\"\n",
    "    See description above\n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    PI = numpy.ndarray[float] - k x 1\n",
    "    k = int\n",
    "    \n",
    "    return:\n",
    "    bayes_info_criterion = int\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_bayes_info(bayes_info_criterion)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4b: Test BIC\n",
    "\n",
    "15 points\n",
    "\n",
    "Now implement `BIC_likelihood_model_test()`, in which you will use the BIC and likelihood to determine the optimal number of components in the `bird_color_24` image. Use the `train_model()` or `train_model_improved()`, iterate from k=2 to k=7 (k - # of clusters) and use the provided means to train a model that minimizes its BIC and a model that maximizes its likelihood.\n",
    "Then, fill out `BIC_likelihood_question()` to return the number of components in both the min-BIC and the max-likelihood model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def BIC_likelihood_model_test():\n",
    "    \"\"\"Test to compare the\n",
    "    models with the lowest BIC\n",
    "    and the highest likelihood.\n",
    "\n",
    "    returns:\n",
    "    (n_comp_min_bic, n_comp_max_likelihood)\n",
    "    n_comp_min_bic = int\n",
    "    n_comp_max_likelihood = int\n",
    "\n",
    "    for testing purposes:\n",
    "    comp_means = [\n",
    "        np.array([[0.67644833, 0.63705593, 0.34151605],\n",
    "               [0.82106333, 0.803725  , 0.38302438]]),\n",
    "        np.array([[0.67644833, 0.63705593, 0.34151605],\n",
    "               [0.82106333, 0.803725  , 0.38302438],\n",
    "               [0.48719558, 0.43396747, 0.57330976]]),\n",
    "        np.array([[0.67644833, 0.63705593, 0.34151605],\n",
    "               [0.82106333, 0.803725  , 0.38302438],\n",
    "               [0.48719558, 0.43396747, 0.57330976],\n",
    "               [0.37512508, 0.7997302 , 0.64795305]]),\n",
    "        np.array([[0.67644833, 0.63705593, 0.34151605],\n",
    "               [0.82106333, 0.803725  , 0.38302438],\n",
    "               [0.48719558, 0.43396747, 0.57330976],\n",
    "               [0.37512508, 0.7997302 , 0.64795305],\n",
    "               [0.14751079, 0.351123  , 0.75471674]]),\n",
    "        np.array([[0.67644833, 0.63705593, 0.34151605],\n",
    "               [0.82106333, 0.803725  , 0.38302438],\n",
    "               [0.48719558, 0.43396747, 0.57330976],\n",
    "               [0.37512508, 0.7997302 , 0.64795305],\n",
    "               [0.14751079, 0.351123  , 0.75471674],\n",
    "               [0.56489468, 0.87895182, 0.25817842]]),\n",
    "        np.array([[0.67644833, 0.63705593, 0.34151605],\n",
    "               [0.82106333, 0.803725  , 0.38302438],\n",
    "               [0.48719558, 0.43396747, 0.57330976],\n",
    "               [0.37512508, 0.7997302 , 0.64795305],\n",
    "               [0.14751079, 0.351123  , 0.75471674],\n",
    "               [0.56489468, 0.87895182, 0.25817842],\n",
    "               [0.08769436, 0.80069854, 0.50162118]])\n",
    "        ]\n",
    "    \"\"\"\n",
    "    # TODO: finish this method\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to use this cell to run your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def BIC_likelihood_question():\n",
    "    \"\"\"\n",
    "    Choose the best number of\n",
    "    components for each metric\n",
    "    (min BIC and maximum likelihood).\n",
    "\n",
    "    returns:\n",
    "    pairs = dict\n",
    "    \"\"\"\n",
    "    # TODO: fill in bic and likelihood\n",
    "    raise NotImplementedError()\n",
    "    bic = 0 \n",
    "    likelihood = 0\n",
    "    pairs = {\n",
    "        'BIC': bic,\n",
    "        'likelihood': likelihood\n",
    "    }\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Return your name\n",
    "\n",
    "1 point\n",
    "\n",
    "A simple task to wind down the assignment. Return your name from the function aptly called `return_your_name()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def return_your_name():\n",
    "    # return your name\n",
    "    # TODO: finish this\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Bonus\n",
    "\n",
    "+5 points\n",
    "\n",
    "#### Bonus points are added to the grade for this assignment, not to your overall grade.\n",
    "\n",
    "A crucial part of machine learning is working with very large datasets. As stated before, using for loops over these datasets will result in the code taking many hours, or even several days, to run. Even vectorization can take time if not done properly, and as such there are certain tricks you can perform to get your code to run as fast as physically possible.\n",
    "\n",
    "For this part of the assignment, you will need to implement part of a k-Means algorithm. You are given two arrays - points_array with X n-dimensional points, and means_array with Y n-dimensional points. You will need to return an X x Y array containing the distances from each point in points_array to each point in means_array.\n",
    "\n",
    "Your code will be tested using two very large arrays, against our reference implementation.\n",
    "\n",
    "If your implementation returns the correct answer in time comparable to our implementation, you will receive 5 bonus points on this assignment.\n",
    "\n",
    "For reference, the data used is in the order of thousands of points and hundreds of means, and Bonnie automatically kills a grading script that takes more than 250MB. So please test accordingly locally before submitting, as you may lose a submission for an inefficient solution. It is very likely that you could run out of memory if your implementation is inefficient.\n",
    "\n",
    "\n",
    "You're done with the requirements! Hope you have completed the functions in the `mixture_models.py` file and tested everything using `mixture_tests.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def bonus(points_array, means_array):\n",
    "    \"\"\"\n",
    "    Return the distance from every point in points_array\n",
    "    to every point in means_array.\n",
    "\n",
    "    returns:\n",
    "    dists = numpy array of float\n",
    "    \"\"\"\n",
    "    # TODO: fill in the bonus function\n",
    "    # REMOVE THE LINE BELOW IF ATTEMPTING BONUS\n",
    "    raise NotImplementedError()\n",
    "    return dists\n",
    "\n",
    "# There are no local test for thus question, fill free to create them yourself.\n",
    "# Feel free to play with it in a separate python file, and then just copy over \n",
    "# your implementation before the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congrats, you are done with the part of the assignment which is graded\n",
    "### Please follow the instructions in the README to submit your code for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is as promised segmentation of the Point Cloud data. \n",
    "\n",
    "In order to run the code below you first need to install an `open3d` library. If you used `pip install -r requirements.txt` command for this assignment, it should already be installed. Else, you can install it `pip install open3d-python` command, if you used virtual environment, make sure to activate it before running the code.\n",
    "\n",
    "You can also refer to official Open3d documentation http://www.open3d.org/docs/getting_started.html for details about the installation and library itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RGBD (**RGB** + **D**epth) data is usually stored as two separated images, one contains RGB (color) information and second one contains only depth, thus is a grayscale image. Let's load a data sample visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open3d import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function below load the data\n",
    "def load_rgbd_image(image_path, depth_path):\n",
    "    color_raw = read_image(image_path)\n",
    "    depth_raw = read_image(depth_path)\n",
    "    #  details about function http://www.open3d.org/docs/tutorial/Basic/rgbd_odometry.html\n",
    "    # We are using a data sample from the SUN RGB-D (http://rgbd.cs.princeton.edu/) dataset\n",
    "    return color_raw, depth_raw\n",
    "\n",
    "# We can plot these images separately using the function below\n",
    "def plot_rgbd(color_image, depth_image):\n",
    "    plt.figure(None,(15,15))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('Color image')\n",
    "    plt.imshow(color_image)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('SUN depth image')\n",
    "    plt.imshow(depth_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "rgbd_dataset = glob.glob('rgbd/image/*.jpg') # TODO fix it\n",
    "image_number = 3 # [0,4] there are five different images in the folder\n",
    "\n",
    "image_file = rgbd_dataset[image_number]\n",
    "depth_file = image_file.replace('image','depth')[:-4] + '.png'\n",
    "assert os.path.isfile(image_file); \n",
    "assert os.path.isfile(depth_file);\n",
    "color_image, depth_image = load_rgbd_image(image_file, depth_file)\n",
    "plot_rgbd(color_image, depth_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we can convert the depth image into a point cloud \n",
    "def show_point_cloud(color_raw, depth_raw):\n",
    "    rgbd_image = create_rgbd_image_from_sun_format(color_raw, depth_raw);\n",
    "    pcd = create_point_cloud_from_rgbd_image(rgbd_image, \n",
    "                 PinholeCameraIntrinsic(PinholeCameraIntrinsicParameters.PrimeSenseDefault))\n",
    "    # Flip it, otherwise the pointcloud will be upside down\n",
    "    pcd.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "    draw_geometries([pcd])\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = show_point_cloud(color_image, depth_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a look at the structure of the point cloud data\n",
    "pcd_points = np.asarray(pcd.points)\n",
    "print(\"Point cloud data - shape:\", pcd_points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point cloud data is represented as an unsorted set of the size M x N., where M is the number of points and N is the x,y,z value for each point. If you are interested you can access the color data in `pcd.colors`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to perform a segmentation on the image we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the number of clusters\n",
    "K = 5\n",
    "# Note: it's just a simple train model run\n",
    "# To improve it you can adapt the best_segment() \n",
    "# to generate the clusters with the best model\n",
    "initial_params = initialize_parameters(pcd_points, K)\n",
    "MU, SIGMA, PI, r = train_model(pcd_points, K,\n",
    "                               convergence_function=default_convergence,\n",
    "                               initial_values=initial_params)\n",
    "clusters = cluster(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of size K of distinct color to plot the clusters\n",
    "# Adapted from https://stackoverflow.com/questions/876853/generating-color-ranges-in-python\n",
    "import colorsys\n",
    "HSV_tuples = [(x*1.0/K, 1.0, 1.0) for x in range(K)]\n",
    "color_maps = list(map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the segmented point cloud data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_pcd = PointCloud() # Create new point cloud handler\n",
    "or_pcd.points = Vector3dVector(pcd_points) # set point cloud data\n",
    "colors = np.zeros_like(pcd_points) # initialize colors to 0\n",
    "for i, point in enumerate(np.unique(clusters)):\n",
    "    random_color = color_maps[i]\n",
    "    cluster_mask = (clusters == point) # get the mask of the cluster i\n",
    "    colors[cluster_mask,:] = random_color # set random color to all the point of this segment\n",
    "or_pcd.colors = Vector3dVector(colors) # set color data\n",
    "draw_geometries([or_pcd]) # visualize point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions to think about:\n",
    "- Would adding a color help or harm the segmentation results?\n",
    "- How about the case: segment RGB data -> add depth -> convert to Point Cloud -> cluster? Would that help/harm?\n",
    "- Could you think of a way you could compress the point cloud data?\n",
    "\n",
    "Things to try:\n",
    "- Segmentation here is done in purely unsupervised manner, you could manually combine multiple gaussian\n",
    "- How about merging multiple scenes into a single one? You could crop one segment from one scene and place it inside another scene.\n",
    "- Try K-means on point cloud data and see what results does it produces\n",
    "- Can we omit the step of conversion to point cloud? And use depth only? Or depth with x,y coordinates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~END~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
