{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISfUJcAkHNbF"
   },
   "source": [
    "# Assignment 5 - Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV2l22o5HNbH"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Automatic image processing is a key component to many AI systems, including facial recognition and video compression, instance segmentation of images and point cloud data. One basic method for processing is segmentation, by which we divide an image into a fixed number of components in order to simplify its representation. For example, we can train a mixture of Gaussians to represent an image, and segment it according to the simplified representation as shown in the images below.\n",
    "\n",
    "![alt text](images/k6_Starry.png)\n",
    "\n",
    "Or we could perform a clustering of point cloud in order to separate different objects, backgrounds etc, as shown in the image below\n",
    "\n",
    "![alt text](images/pcd_clustered.gif)\n",
    "\n",
    "In this assignment, you will learn to perform image compression and point cloud segmentation. To this end, you will implement Gaussian mixture models and iteratively improve their performance. First you will perform segmentation on the \"Starry\" (`Starry.png`) and at the end run your algorithm on 3D point cloud data.\n",
    "\n",
    "To begin, you will implement several methods of image segmentation, with increasing complexity:\n",
    "\n",
    "1. Implement k-means clustering to segment a color image.\n",
    "\n",
    "2. Familiarize yourself with the algorithm by running it on simple dataset.\n",
    "\n",
    "3. Build a Gaussian mixture model to be trained with expectation-maximization.\n",
    "\n",
    "4. Experiment with varying the details of the Gaussian mixture modelâ€™s implementation.\n",
    "\n",
    "5. Implement and test a new metric called the Bayesian information criterion, which guarantees a more robust image segmentation.\n",
    "\n",
    "<br>\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3-iorXnHNbI"
   },
   "source": [
    "## Part 0: Note on Vectorization\n",
    "\n",
    "The concept of Vectorization was introduced in the last section of Assignment 4. For this assignment, please vectorize your code wherever possible using numpy arrays, instead of running for-loops over the images being processed.\n",
    "\n",
    "For example of how this might be useful, consider the following array:\n",
    "A = [12 34 1234 764 ...(has a million values)... 91, 78]\n",
    "\n",
    "Now you need to calculate another array B, which has the same dimensions as A above. Say each value in B is calculated as follows:\n",
    "(each value in B) = square_root_of(some constants pi log(k) * (each value in A))/7\n",
    "\n",
    "You might wish to use a for-loop to compute this. However, it will take really long to run on an array of this magnitude.\n",
    "Alternatively, you may choose to use numpy and perform this calculation in a single line. You can pass A as a numpy array and the entire calculation will be done in a line, resulting in B being populated with the corresponding values that come out of this formula.\n",
    "\n",
    "Check out **Basic Operation** section of the Numpy Tutorial if you are not familiar with numpy vector/matrix operations: https://docs.scipy.org/doc/numpy/user/quickstart.html#basic-operations\n",
    "\n",
    "#### Please note that numpy.vectorize DOES NOT perform vectorization, it only does a loop. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUhnhEdvHNbJ"
   },
   "source": [
    "## Part 1: K-means Clustering (19 pts)\n",
    "\n",
    "One easy method for image segmentation is to simply cluster all similar data points together and then replace their values with the mean value. Thus, we'll warm up using k-means clustering. This will also provide a baseline to compare with your segmentation. Please note that clustering will come in handy later.\n",
    "\n",
    "Fill out `get_initial_means()`, `k_means_step()` functions below.\n",
    "\n",
    "In `get_initial_means()`, you should choose  k random points from the data (without replacement) to use as initial cluster means.\n",
    "\n",
    "Your code will be unit tested automatically when you run the cell (`Cell > Run Cells OR Shift + Enter`).\n",
    "\n",
    "#### Try to vectorize the code for it to run faster. Without vectorization it takes 25-30 minutes for the code to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHRbOkJnHNbJ",
    "outputId": "3405377b-cd38-4bba-d253-75d7a7163353"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Run this cell and check if you have all necessary modules\n",
    "from ipywidgets import *\n",
    "import mixture_tests as tests\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as pat\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import numpy as np\n",
    "from helper_functions import *\n",
    "# Please don't modify this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjJX8ByfHNbN"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psHj3p1wHNbQ",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def get_initial_means(array, k):\n",
    "    \"\"\"\n",
    "    Picks k random points from the 2D array \n",
    "    (without replacement) to use as initial \n",
    "    cluster means\n",
    "\n",
    "    params:\n",
    "    array = numpy.ndarray[numpy.ndarray[float]] - m x n | datapoints x features\n",
    "\n",
    "    k = int\n",
    "\n",
    "    returns:\n",
    "    initial_means = numpy.ndarray[numpy.ndarray[float]]\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.K_means_test().test_initial_means(get_initial_means)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_txM46_YHNbT",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def k_means_step(X, k, means):\n",
    "    \"\"\"\n",
    "    A single update/step of the K-means algorithm\n",
    "    Based on a input X and current mean estimate,\n",
    "    predict clusters for each of the pixels and \n",
    "    calculate new means. \n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n | pixels x features (already flattened)\n",
    "    k = int\n",
    "    means = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "\n",
    "    returns:\n",
    "    (new_means, clusters)\n",
    "    new_means = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    clusters = numpy.ndarray[int] - m sized vector\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.K_means_test().test_k_means_step(k_means_step)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkmRHcLeHNbW"
   },
   "source": [
    "#### K-means - Visualizing the results\n",
    "\n",
    "Now that you are done with the K-means step implementation lets try to visualize what's happening if you repeat these steps multiple times.\n",
    "\n",
    "**You don't need to be implementing anything in the next cells until Image Segmentation Section**, but you are highly encouraged to play with parameters and datasets, to get a sense of what is happening at every algorithm iteration step.\n",
    "\n",
    "Feel free to explore and improve the function below, it will be used for visualizing K-means progress\n",
    "but it's not required and WON'T effect your grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-r7LryvHNbW"
   },
   "outputs": [],
   "source": [
    "# This cell contains a code for loading a dataset from the `data` folder\n",
    "# Each of these datasets contains synthtic (generated) data\n",
    "# You can simply run this cell for now and come back to it later if you want to make changes\n",
    "# Make sure you implemented everything in cells above and passed the unittests\n",
    "def K_means_2D_dataset(dataset_index, K):\n",
    "    # Load the dataset from data folder\n",
    "    X = np.loadtxt(\"data/%d_dataset_X.csv\" % dataset_index, delimiter=\",\")\n",
    "    print(\"The dataset is of a size:\", X.shape)\n",
    "\n",
    "    # Load the labels\n",
    "    # Clustering is unsupervised method, where no labels are provided\n",
    "    # However, since we generated the data outselves we know the clusters,\n",
    "    # and load them for illustration purposes.\n",
    "    y = np.int16(np.loadtxt(\"data/%d_dataset_y.csv\" % dataset_index, delimiter=\",\"))\n",
    "\n",
    "    # Feel free to edit the termination condition for the K-means algorithm\n",
    "    # Currently is just runs for n_iterations, before terminating\n",
    "    n_iterations = 10\n",
    "    m,n = X.shape\n",
    "    means = get_initial_means(X,K)\n",
    "    clusters = np.zeros([n])\n",
    "    # keeping track of how clusters and means changed, for visualization purposes\n",
    "    means_history = [means]\n",
    "    clusters_history = [clusters] \n",
    "    for iteration_i in range(n_iterations):\n",
    "        means, clusters = k_means_step(X, K, means)\n",
    "        clusters_history.append(clusters)\n",
    "\n",
    "    return X, y, means_history, clusters_history\n",
    "\n",
    "# Things to try:\n",
    "# - Try different initialization to see check initialization robustness\n",
    "# - Improve the termination condition (you will be able to reuse later as well!)\n",
    "# - Try creating you own dataset in the `data/` folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weDmGThLHNbY"
   },
   "outputs": [],
   "source": [
    "# RUN - TRY DIFFERENT PARAMETERS - REPEAT\n",
    "dataset_index = 2 # for different dataset change it to number from [0,4]\n",
    "K = 5 # Number of clusters - play with this number\n",
    "\n",
    "X, y, means_history, clusters_history = K_means_2D_dataset(dataset_index, K)\n",
    "\n",
    "# This is an interactive cell to see the progress of training your K-means algorithm.\n",
    "# Feel free to improve the visualization code and share it with your classmates on Piazza\n",
    "def get_cluster(i):\n",
    "    clusters = clusters_history[i] # Get the clusters from K-means' i-th iteration\n",
    "    plt.figure(None, figsize=(15,6)) # Set the plot size\n",
    "    plt.suptitle('Drag the slider to see the algorthm training progress')\n",
    "    ax1=plt.subplot(1, 2, 1)\n",
    "    ax1.set_title('K-means clusters - step %d' % i)\n",
    "    for k in range(K):\n",
    "        plt.plot(X[clusters==k,0], X[clusters==k,1], '.') # \n",
    "        # Try to plot the centers of the clusters \n",
    "        # You can access them by calling means_history[i]\n",
    "        # How could you plot the area that belong to that cluster?\n",
    "\n",
    "    # Just to get a flavour of how the data looks like\n",
    "    ax2=plt.subplot(1, 2, 2)\n",
    "    ax2.set_title('Ground truth clusters')\n",
    "    for i in np.unique(y):\n",
    "        ax2.plot(X[y==i,0],X[y==i,1],'.')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "interactive(get_cluster, i=(1,len(clusters_history)-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-iiPyzuHNbd"
   },
   "source": [
    "### Image segmentation\n",
    "2D data clustering is all cool and all but now it's time to use K-means for the image compression! \n",
    "\n",
    "Fill in the `k_means_segment()` function below, you will find your `k_means_step()` and `get_initial_means()` very handy here. \n",
    "\n",
    "You will separate the provided RGB values into k clusters using the k-means algorithm, then return an updated version of the image with the original values replaced with the corresponding cluster center values.\n",
    "\n",
    "Your convergence test should be whether the assigned clusters stop changing. Note that this convergence test is rather slow. When no initial cluster means (`initial_means`) are provided, you need to initialize them yourself, based on the given k.\n",
    "\n",
    "For this part of the assignment, since clustering is best used on multidimensional data, we will be using the color image `Starry.png`.\n",
    "\n",
    "Please pay close attention to the dimensions of the data. In the `k_means_step()` you were working with m x n data, here the input is an image (`image_values`) which has a shape of **rows x columns x color_channels**.\n",
    "\n",
    "The function should return an updated version of the image with the original values replaced with the corresponding cluster values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9XUDTQHHNbd",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def k_means_segment(image_values, k=3, initial_means=None):\n",
    "    \"\"\"\n",
    "    Separate the provided RGB values into\n",
    "    k separate clusters using the k-means algorithm,\n",
    "    then return an updated version of the image\n",
    "    with the original values replaced with\n",
    "    the corresponding cluster values.\n",
    "\n",
    "    params:\n",
    "    image_values = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - r x c x ch\n",
    "    k = int\n",
    "    initial_means = numpy.ndarray[numpy.ndarray[float]] or None\n",
    "\n",
    "    returns:\n",
    "    updated_image_values = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - r x c x ch\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.K_means_test().test_k_means(k_means_segment)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKU3OdmwHNbf"
   },
   "source": [
    "### Visulizing K-means segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbj3ETyrHNbi"
   },
   "outputs": [],
   "source": [
    "k=5 # number of clusters - feel free to play with it\n",
    "\n",
    "image_values = image_to_matrix('images/Starry.png')\n",
    "# Play with the K value below to see the effect number of clusters have\n",
    "new_image = k_means_segment(image_values, k=k)\n",
    "\n",
    "plt.figure(None,figsize=(9,12))\n",
    "plt.imshow(new_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ty-hePOgHNb1"
   },
   "source": [
    "\n",
    "You can reuse the K-means visualization code from previous section to show the training progress on the image for different iterations and even numbers of clusters.\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-EVOn9nHNb2"
   },
   "source": [
    "## Part 2: Implementing a Multivariate Gaussian Mixture Model (48 pts)\n",
    "\n",
    "Next, we will step beyond clustering and implement a complete Gaussian mixture model.\n",
    "\n",
    "But, before you dive into the code, you are highly encouraged to go over `read/gaussians.pdf` file before you start, to familiarize yourself with multivariate case of the Gaussian distribution.\n",
    "\n",
    "In addition to that, there is a great ~17 minute video where Alexander Ihler goes over nuts and bolds of the multivariate EM algorithm details on Youtube:\n",
    "https://www.youtube.com/watch?v=qMTuMa86NzU\n",
    "\n",
    "Another resource you can refer to is the `read/em.pdf` document attached, which is a chapter from Pattern Recognition and Machine Learning book by Christopher M. Bishop.\n",
    "\n",
    "- - - \n",
    "\n",
    "Now, it's time to complete the implementation of the functions below what will later assemble into a Multivariate Gaussian Expectation Maximization algorithm:\n",
    "\n",
    "1. Calculate the probability of a given data point (e.g. rgb value of a pixel) of belonging to a specific Gaussian component. (7 points)\n",
    "\n",
    "2. Use expectation-maximization (EM) to train the model to represent the image as a mixture of Gaussians. (20 points)\n",
    "\n",
    "To initialize EM, set each component's mean to the means value of randomly chosen pixels (same as for K-means) and calculate covariances based on the selected means, and set the mixing coefficients to a uniform distribution. \n",
    "\n",
    "We've set the convergence condition for you in `default_convergence()` (see `helper_functions.py` file): if the new likelihood is within 10% of the previous likelihood for 10 consecutive iterations, the model has converged.\n",
    "\n",
    "**Note:** there are packages that can run EM automagically, but you have to implement your own version of EM without using these extra packages. **It also means that you are not allowed to look into any implementations of the algorithms, e.g scikit-learn and many others. NumPy is your only tool here.** \n",
    "\n",
    "3. Calculate the log likelihood of the trained model. (7 points)\n",
    "4. Segment the image according to the trained model. (7 points)\n",
    "5. Determine the best segmentation by iterating over model training and scoring, since EM isn't guaranteed to converge to the global maximum. (7 points)\n",
    "\n",
    "It'd be helpful to implement the above functions in the following order - \n",
    "1. initialize_parameters\n",
    "2. prob\n",
    "3. E_step\n",
    "4. M_step\n",
    "5. likelihood \n",
    "6. train_model\n",
    "7. cluster\n",
    "8. segment\n",
    "9. best_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6q60JTOmHNb2"
   },
   "source": [
    "### Warning: You may lose all marks for this part if your code runs for too long.\n",
    "\n",
    "**You will need to vectorize your code in this part. Specifically, the method E_step() and M_step() which make up the train_model(), perform operations using numpy arrays. These are time-sensitive functions and will be called over and over as you proceed with this assignment.**\n",
    "\n",
    "For the synthetic data test which we provide to check if your training is working, the set is too small and it won't make a difference. But with the actual image that we use ahead, for-loops won't do good. Vectorized code would take under 30 seconds to converge which would typically involve about 15-20 iterations with the convergence function we have here. Inefficient code that uses loops or iterates over each pixel value sequentially, will take hours to run. You don't want to do that.\n",
    "\n",
    "- - -\n",
    "\n",
    "Same as in K-means you will be working with the data of size (m x n). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzlJezxTHNb3",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def initialize_parameters(X, k):\n",
    "    \"\"\"\n",
    "    Return initial values for training of the GMM\n",
    "    Set component mean to a random\n",
    "    pixel's value (without replacement),\n",
    "    based on the mean calculate covariance matrices,\n",
    "    and set each component mixing coefficient (PIs)\n",
    "    to a uniform values\n",
    "    (e.g. 4 components -> [0.25,0.25,0.25,0.25]).\n",
    "    \n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    k = int\n",
    "    \n",
    "    returns:\n",
    "    (MU, SIGMA, PI)\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    PI = numpy.ndarray[float] - k x 1 \n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_initialization(initialize_parameters)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdcZyf4tHNb7"
   },
   "source": [
    "The following cell (compute_sigma) will not be graded, but we highly recommend using this function and paired test to make sure your covariance matrix implementation is correct. Computing the covariance matrix incorrectly can result in problems that become extremely hard to debug later in the assignment so please take advantage of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtWDrD6wHNb8"
   },
   "outputs": [],
   "source": [
    "def compute_sigma(X, MU):\n",
    "    \"\"\"\n",
    "    Calculate covariance matrix, based in given X and MU values\n",
    "    \n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    \n",
    "    returns:\n",
    "    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_covariance(compute_sigma)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1II4ri1THNcD"
   },
   "source": [
    "# NOTE:\n",
    "\n",
    "### Be careful when coding up prob() below. It is fine for prob() to take the vectorized approach, but you may have to adjust your implementation to handle both cases. Specifically the case where x is a single datapoint and where x is an entire array of datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKudaHqgHNcE",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def prob(x, mu, sigma):\n",
    "    \"\"\"Calculate the probability of x (a single\n",
    "    data point or an array of data points) under the\n",
    "    component with the given mean and covariance.\n",
    "    The function is intended to compute multivariate\n",
    "    normal distribution, which is given by N(x;MU,SIGMA).\n",
    "\n",
    "    params:\n",
    "    x = numpy.ndarray[float] or numpy.ndarray[numpy.ndarray[float]]\n",
    "    mu = numpy.ndarray[float]\n",
    "    sigma = numpy.ndarray[numpy.ndarray[float]]\n",
    "\n",
    "    returns:\n",
    "    probability = float or numpy.ndarray[float]\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_prob(prob)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4Zd5XXgHNcJ",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def E_step(X,MU,SIGMA,PI,k):\n",
    "    \"\"\"\n",
    "    E-step - Expectation \n",
    "    Calculate responsibility for each\n",
    "    of the data points, for the given \n",
    "    MU, SIGMA and PI.\n",
    "    \n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    PI = numpy.ndarray[float] - k x 1\n",
    "    k = int\n",
    "    \n",
    "    returns:\n",
    "    responsibility = numpy.ndarray[numpy.ndarray[float]] - k x m\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_e_step(E_step)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IU6IEwZHHNcM",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def M_step(X, r, k):\n",
    "    \"\"\"\n",
    "    M-step - Maximization\n",
    "    Calculate new MU, SIGMA and PI matrices\n",
    "    based on the given responsibilities.\n",
    "    \n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    r = numpy.ndarray[numpy.ndarray[float]] - k x m\n",
    "    k = int\n",
    "    \n",
    "    returns:\n",
    "    (new_MU, new_SIGMA, new_PI)\n",
    "    new_MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    new_SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    new_PI = numpy.ndarray[float] - k x 1\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_m_step(M_step)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQ1n7pSCHNcO",
    "scrolled": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def likelihood(X, PI, MU, SIGMA, k):\n",
    "    \"\"\"Calculate a log likelihood of the \n",
    "    trained model based on the following\n",
    "    formula for posterior probability:\n",
    "    \n",
    "    log(Pr(X | mixing, mean, stdev)) = sum((i=1 to m), log(sum((j=1 to k),\n",
    "                                      mixing_j * N(x_i | mean_j,stdev_j))))\n",
    "\n",
    "    Make sure you are using natural log, instead of log base 2 or base 10.\n",
    "    \n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    PI = numpy.ndarray[float] - k x 1\n",
    "    k = int\n",
    "\n",
    "    returns:\n",
    "    log_likelihood = float\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_likelihood(likelihood)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qntf9HuFHNcV",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def train_model(X, k, convergence_function, initial_values = None):\n",
    "    \"\"\"\n",
    "    Train the mixture model using the \n",
    "    expectation-maximization algorithm. \n",
    "    E.g., iterate E and M steps from \n",
    "    above until convergence.\n",
    "    If the initial_values are None, initialize them.\n",
    "    Else it's a tuple of the format (MU, SIGMA, PI).\n",
    "    Convergence is reached when convergence_function\n",
    "    returns terminate as True,\n",
    "    see default convergence_function example \n",
    "    in `helper_functions.py`\n",
    "\n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    k = int\n",
    "    convergence_function = func\n",
    "    initial_values = None or (MU, SIGMA, PI)\n",
    "\n",
    "    returns:\n",
    "    (new_MU, new_SIGMA, new_PI, responsibility)\n",
    "    new_MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    new_SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    new_PI = numpy.ndarray[float] - k x 1\n",
    "    responsibility = numpy.ndarray[numpy.ndarray[float]] - k x m\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_train(train_model, likelihood)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnEL9hjXHNcX",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def cluster(r):\n",
    "    \"\"\"\n",
    "    Based on a given responsibilities matrix\n",
    "    return an array of cluster indices.\n",
    "    Assign each datapoint to a cluster based,\n",
    "    on component with a max-likelihood \n",
    "    (maximum responsibility value).\n",
    "    \n",
    "    params:\n",
    "    r = numpy.ndarray[numpy.ndarray[float]] - k x m - responsibility matrix\n",
    "    \n",
    "    return:\n",
    "    clusters = numpy.ndarray[int] - m x 1 \n",
    "    \"\"\"\n",
    "    # TODO: finish this\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_cluster(cluster)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paVJfoIoHNca",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def segment(X, MU, k, r):\n",
    "    \"\"\"\n",
    "    Segment the X matrix into k components. \n",
    "    Returns a matrix where each data point is \n",
    "    replaced with its max-likelihood component mean.\n",
    "    E.g., return the original matrix where each pixel's\n",
    "    intensity replaced with its max-likelihood \n",
    "    component mean. (the shape is still mxn, not \n",
    "    original image size)\n",
    "\n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    k = int\n",
    "    r = numpy.ndarray[numpy.ndarray[float]] - k x m - responsibility matrix\n",
    "\n",
    "    returns:\n",
    "    new_X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_segment(train_model, segment)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3FehT_rHNcd",
    "scrolled": true,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def best_segment(X,k,iters):\n",
    "    \"\"\"Determine the best segmentation\n",
    "    of the image by repeatedly\n",
    "    training the model and\n",
    "    calculating its likelihood.\n",
    "    Return the segment with the\n",
    "    highest likelihood.\n",
    "\n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    k = int\n",
    "    iters = int\n",
    "\n",
    "    returns:\n",
    "    (likelihood, segment)\n",
    "    likelihood = float\n",
    "    segment = numpy.ndarray[numpy.ndarray[float]]\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_best_segment(best_segment)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZZIUtr1HNcf"
   },
   "source": [
    "#### GMM - Visualizing the results\n",
    "\n",
    "Now that you are done with the EM implementation lets try to visualize what's happening if you repeat these steps multiple times.\n",
    "\n",
    "**You don't need to be implementing anything in the next 2 cells, but you are highly encouraged to play with parameters and datasets, to get a visual sense of what is happening at every step.**\n",
    "\n",
    "\n",
    "Feel free to explore and improve the function below, it will be used for visualizing GMM progress\n",
    "but it's not required and WON'T effect your grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h27gArYPHNcf"
   },
   "outputs": [],
   "source": [
    "def GMM_2D_dataset(dataset_index, K):\n",
    "    # Load the dataset from data folder\n",
    "    X = np.loadtxt(\"data/%d_dataset_X.csv\" % dataset_index, delimiter=\",\")\n",
    "    print(\"There are %d datapoints in the current dataset, each of a size %d\" % X.shape)\n",
    "    print(\"\"\"\\nNote that that the Gaussian Ellipses and Normal Curves may not share the\n",
    "same color as the points they represent (within the same chart).\n",
    "In fact, the Gaussian Ellipses and Normal Curves represent the clusters\n",
    "in the top left chart (and thus share colors with those points).\"\"\")\n",
    "    # Load the labels\n",
    "    # Clustering is unsupervised method, where no labels are provided\n",
    "    # However, since we generated the data outselves we know the labels,\n",
    "    # and load them for illustration purposes.\n",
    "    y = np.int16(np.loadtxt(\"data/%d_dataset_y.csv\" % dataset_index, delimiter=\",\"))\n",
    "    # Feel free to edit the termination condition for the EM algorithm\n",
    "    # Currently is just runs for n_iterations, before terminating\n",
    "    \n",
    "    MU, SIGMA, PI = initialize_parameters(X, K)\n",
    "    \n",
    "    clusters_history = []\n",
    "    statistics_history = []\n",
    "    for _ in range(200):\n",
    "        r = E_step(X,MU,SIGMA,PI,K)\n",
    "        new_MU, new_SIGMA, new_PI = M_step(X, r, K)\n",
    "        PI, MU, SIGMA = new_PI, new_MU, new_SIGMA\n",
    "        clusters = cluster(r)\n",
    "        clusters_history.append(clusters)\n",
    "        statistics_history.append((PI, MU, SIGMA))\n",
    "\n",
    "    return X, y, clusters_history, statistics_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCt92ePHHNch"
   },
   "outputs": [],
   "source": [
    "# TRY DIFFERENT PARAMETERS\n",
    "dataset_index = 3 # for different dataset change it to number from [0,5]\n",
    "K = 3 # Number of clusters - play with this number\n",
    "\n",
    "X, y, clusters_history, statistics_history = GMM_2D_dataset(dataset_index, K)\n",
    "\n",
    "def setup_subplot(plt, i, title, plot_number):\n",
    "    ax = plt.subplot(2, 2, plot_number)\n",
    "    ax.set_title(title)\n",
    "    ax.patch.set_facecolor('gray')\n",
    "    ax.patch.set_alpha(0.1)\n",
    "    return ax\n",
    "\n",
    "def plot_gaussian_ellipse(k, mean, covar, ax2, colors):\n",
    "    v,w = np.linalg.eig(covar)\n",
    "        \n",
    "    angle = np.arctan(w[1,0] / w[0,0])\n",
    "    angle = 180 * angle / np.pi\n",
    "    \n",
    "    color = colors[k % len(colors)]\n",
    "    for i in range(3,8):\n",
    "        plot_v = i * np.sqrt(v)\n",
    "        ellipse = pat.Ellipse(mean, plot_v[0], plot_v[1], angle, fill = True, alpha = 0.10, lw = 1.0, ls = 'dashdot', ec = 'black', fc = color, zorder = 0)\n",
    "        ax2.add_artist(ellipse)\n",
    "\n",
    "def plot_gaussian(X, mean, var, X_min, X_max, ax):\n",
    "    samples = np.linspace(X_min, X_max, 100)\n",
    "    ax.plot(samples, norm.pdf(samples, mean, var))\n",
    "    \n",
    "\n",
    "# This is an interactive cell to see the progress of training your GMM algorithm.\n",
    "# Feel free to improve the visualization code and share it with your classmates on Piazza.\n",
    "def get_cluster(i):\n",
    "    clusters = clusters_history[i] # Get the clusters from K-means' i-th iteration\n",
    "    cluster_means = statistics_history[i][1]\n",
    "    cluster_covar = statistics_history[i][2]\n",
    "    \n",
    "    plt.figure(None, figsize=(15,12)) # Set the plot size\n",
    "    plt.suptitle('Drag the slider to see the algorithm training progress')\n",
    "    \n",
    "    ax1 = setup_subplot(plt, i, 'GMM clusters - step %d' % i, 1)\n",
    "    ax2 = setup_subplot(plt, i, 'Ground truth clusters', 2)\n",
    "    ax3 = setup_subplot(plt, i, 'GMM Gausians X1 - step %d' % i, 3)\n",
    "    ax4 = setup_subplot(plt, i, 'GMM Gausians X2 - step %d' % i, 4)\n",
    "    \n",
    "    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    for k in range(K):\n",
    "        ax1.plot(X[clusters==k, 0], X[clusters==k, 1], '.')\n",
    "        \n",
    "        mean = cluster_means[k]\n",
    "        covar = cluster_covar[k]\n",
    "        \n",
    "        plot_gaussian_ellipse(k, mean, covar, ax2, colors)\n",
    "        plot_gaussian(X[clusters==k, 0], mean[0], covar[0,0], np.min(X[:, 0]), np.max(X[:, 0]), ax3)\n",
    "        plot_gaussian(X[clusters==k, 1], mean[1], covar[1,1], np.min(X[:, 1]), np.max(X[:, 1]), ax4)\n",
    "    \n",
    "    ax3.set_prop_cycle(None)\n",
    "    ax4.set_prop_cycle(None)\n",
    "    # Just to get a flavour of how the data looks like\n",
    "    for i in np.unique(y):\n",
    "        ax2.plot(X[y==i,0], X[y==i,1],'.', zorder=10)\n",
    "        ax3.plot(X[y==i,0], np.zeros(X[y==i,0].shape[0]), '.', zorder = 10)\n",
    "        ax4.plot(X[y==i,1], np.zeros(X[y==i,1].shape[0]), '.', zorder = 10)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "interactive(get_cluster, {'manual': True}, i=(0,len(clusters_history)-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OG0eYKOHNcj"
   },
   "source": [
    "### Let's visualize the image compression results of GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MH8wSDcXHNcj"
   },
   "outputs": [],
   "source": [
    "image_file = 'images/Starry.png' # Image path\n",
    "original_image_matrix = image_to_matrix(image_file) # Save original image\n",
    "image_matrix = original_image_matrix.reshape(-1,3) # collapse the dimension\n",
    "K = 10 # K\n",
    "\n",
    "_, best_seg = best_segment(image_matrix, K, iters = 10)\n",
    "new_image = best_seg.reshape(*original_image_matrix.shape) # reshape collapsed matrix to original size\n",
    "# Show the image\n",
    "plt.figure(None,figsize=(9,12))\n",
    "plt.imshow(new_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crCeltr8HNcl"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPRFQJWkHNcl"
   },
   "source": [
    "## Part 3: Model Experimentation (20 pts)\n",
    "\n",
    "We'll now experiment with a few methods for improving GMM performance.\n",
    "\n",
    "## Part 3a: Improved Initialization \n",
    "\n",
    "#### 12.5 points\n",
    "\n",
    "To run EM in our baseline Gaussian mixture model, we use random initialization to determine the initial values for our component means. We can do better than this!\n",
    "\n",
    "Fill in `improved_initialization()` by training a GMM to find initial means. This type of initialization differs from simply increasing training time because we \"reset\" the covariance and mixing coefficient parameters. That is, for training, we recompute covariance parameters based on the means we learned during initialization and again set the mixing coefficients to a uniform distribution. A GMM tends to converge to elongated covariances, so by resetting these parameters we have a higher chance of avoiding local maxima.\n",
    "\n",
    "Please don't use any external packages for anything other than basic calculations. Note that your improvement might significantly slow down runtime, although we don't expect you to spend more than 10 minutes on initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ds6GlJZVHNcm",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def improved_initialization(X,k):\n",
    "    \"\"\"\n",
    "    Initialize the training\n",
    "    process by setting each\n",
    "    component mean using some algorithm that\n",
    "    you think might give better means to start with,\n",
    "    based on the mean calculate covariance matrices,\n",
    "    and set each component mixing coefficient (PIs)\n",
    "    to a uniform values\n",
    "    (e.g. 4 components -> [0.25,0.25,0.25,0.25]).\n",
    "    \n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    k = int\n",
    "    \n",
    "    returns:\n",
    "    (MU, SIGMA, PI)\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    PI = numpy.ndarray[float] - k x 1 \n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_gmm_improvement(improved_initialization, initialize_parameters, train_model, likelihood)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-IPuhwnHNco"
   },
   "source": [
    "## Part 3b: Convergence Condition\n",
    "\n",
    "#### 7.5 points\n",
    "\n",
    "You might be skeptical of the convergence criterion we've provided in `default_convergence()`. To test out another convergence condition, implement `new_convergence_condition()` to return true if all the new model parameters (means, variances, and mixing coefficients) are within 10% of the previous variables for 10 consecutive iterations. This will mean re-implementing `train_model()` in the `train_model_improved()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSTjxCExHNco",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def new_convergence_function(previous_variables, new_variables, conv_ctr,\n",
    "                             conv_ctr_cap=10):\n",
    "    \"\"\"\n",
    "    Convergence function\n",
    "    based on parameters:\n",
    "    when all variables vary by\n",
    "    less than 10% from the previous\n",
    "    iteration's variables, increase\n",
    "    the convergence counter.\n",
    "\n",
    "    params:\n",
    "    previous_variables = [numpy.ndarray[float]]\n",
    "                         containing [means, variances, mixing_coefficients]\n",
    "    new_variables = [numpy.ndarray[float]]\n",
    "                    containing [means, variances, mixing_coefficients]\n",
    "    conv_ctr = int\n",
    "    conv_ctr_cap = int\n",
    "\n",
    "    return:\n",
    "    (conv_crt, converged)\n",
    "    conv_ctr = int\n",
    "    converged = boolean\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def train_model_improved(X, k, convergence_function, initial_values = None):\n",
    "    \"\"\"\n",
    "    Train the mixture model using the \n",
    "    expectation-maximization algorithm. \n",
    "    E.g., iterate E and M steps from \n",
    "    above until convergence.\n",
    "    If the initial_values are None, initialize them.\n",
    "    Else it's a tuple of the format (MU, SIGMA, PI).\n",
    "    Convergence is reached when convergence_function\n",
    "    returns terminate as True. Use new_convergence_fuction \n",
    "    implemented above. \n",
    "\n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    k = int\n",
    "    convergence_function = func\n",
    "    initial_values = None or (MU, SIGMA, PI)\n",
    "\n",
    "    returns:\n",
    "    (new_MU, new_SIGMA, new_PI, responsibility)\n",
    "    new_MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    new_SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    new_PI = numpy.ndarray[float] - k x 1\n",
    "    responsibility = numpy.ndarray[numpy.ndarray[float]] - k x m\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "# Unittest below will check both of the functions at the same time. \n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_convergence_condition(improved_initialization, train_model_improved, initialize_parameters, train_model, likelihood, new_convergence_function)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpokxcYDHNcs"
   },
   "source": [
    "## Part 4: Bayesian Information Criterion (12 pts)\n",
    "\n",
    "In our previous solutions, our only criterion for choosing a model was whether it maximizes the posterior likelihood regardless of how many parameters this requires. As a result, the \"best\" model may simply be the model with the most parameters, which would be overfit to the training data.\n",
    "\n",
    "To avoid overfitting, we can use the [Bayesian information criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion) (a.k.a. BIC) which penalizes models based on the number of parameters they use. In the case of the Gaussian mixture model, this is equal to the number of components times the number of variables per component (mean, variance and mixing coefficient).\n",
    "\n",
    "## Part 4a: Implement BIC\n",
    "\n",
    "#### 4 points\n",
    "\n",
    "Implement `bayes_info_criterion()` to calculate the BIC of a trained Gaussian Mixture Model (based on the given parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWWWYvXcHNct",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def bayes_info_criterion(X, PI, MU, SIGMA, k):\n",
    "    \"\"\"\n",
    "    See description above\n",
    "    params:\n",
    "    X = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    MU = numpy.ndarray[numpy.ndarray[float]] - k x n\n",
    "    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n\n",
    "    PI = numpy.ndarray[float] - k x 1\n",
    "    k = int\n",
    "    \n",
    "    return:\n",
    "    bayes_info_criterion = int\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.GMMTests().test_bayes_info(bayes_info_criterion)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01U3ixAdHNcv"
   },
   "source": [
    "## Part 4b: Test BIC\n",
    "\n",
    "#### 8 points\n",
    "\n",
    "Now implement `BIC_likelihood_model_test()`, in which you will use the BIC and likelihood to determine the optimal number of components in the `image_matrix` parameter. Using `train_model()` or `train_model_improved()`, iterate over the list of provided means (`comp_means`) to train a model that minimizes its BIC and a model that maximizes its likelihood. \n",
    "\n",
    "Return:\n",
    "\n",
    "1) The number of components which result in the minimum BIC\n",
    "\n",
    "2) The number of components which result in the highest likelihood\n",
    "\n",
    "`comp_means` is a list, where each element is a k x n matrix of means (where k = # of clusters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExKFgbSCHNcw",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def BIC_likelihood_model_test(image_matrix, comp_means):\n",
    "    \"\"\"Returns the number of components\n",
    "    corresponding to the minimum BIC \n",
    "    and maximum likelihood with respect\n",
    "    to image_matrix and comp_means.\n",
    "    \n",
    "    params:\n",
    "    image_matrix = numpy.ndarray[numpy.ndarray[float]] - m x n\n",
    "    comp_means = list(numpy.ndarray[numpy.ndarray[float]]) - list(k x n) (means for each value of k)\n",
    "\n",
    "    returns:\n",
    "    (n_comp_min_bic, n_comp_max_likelihood)\n",
    "    n_comp_min_bic = int\n",
    "    n_comp_max_likelihood = int\n",
    "    \"\"\"\n",
    "    # TODO: finish this method\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLCUO6h-HNcy"
   },
   "source": [
    "## Part 5: Return your name\n",
    "\n",
    "#### 1 point\n",
    "\n",
    "A simple task to wind down the assignment. Return your name from the function aptly called `return_your_name()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sr_9uWzOHNcy",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def return_your_name():\n",
    "    # return your name\n",
    "    # TODO: finish this\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywkOL8FiHNc5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLHrqPFTHNc5"
   },
   "source": [
    "## Congrats, you are done with the part of the assignment which is graded\n",
    "### Please follow the instructions in the README to submit your code for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2aY5pZZHNc6"
   },
   "source": [
    "- - -\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yVfXIqIHNc6"
   },
   "source": [
    "Next is as promised segmentation of the Point Cloud data. \n",
    "\n",
    "If you run into issues with `open3d` library below, please refer to official Open3d documentation http://www.open3d.org/docs/getting_started.html for details about the installation and library itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U58QXlMxHNc7"
   },
   "source": [
    "RGBD (**RGB** + **D**epth) data is usually stored as two separated images, one contains RGB (color) information and second one contains only depth, thus is a grayscale image. Let's load a data sample visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4egnutjHNc7"
   },
   "outputs": [],
   "source": [
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPzqXL03HNdG"
   },
   "outputs": [],
   "source": [
    "# Function below load the data\n",
    "def load_rgbd_image(image_path, depth_path):\n",
    "    color_raw = o3d.io.read_image(image_path)\n",
    "    depth_raw = o3d.io.read_image(depth_path)\n",
    "    #  details about function http://www.open3d.org/docs/tutorial/Basic/rgbd_odometry.html\n",
    "    # We are using a data sample from the SUN RGB-D (http://rgbd.cs.princeton.edu/) dataset\n",
    "    return color_raw, depth_raw\n",
    "\n",
    "# We can plot these images separately using the function below\n",
    "def plot_rgbd(color_image, depth_image):\n",
    "    plt.figure(None,(15,15))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('Color image')\n",
    "    plt.imshow(color_image)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('SUN depth image')\n",
    "    plt.imshow(depth_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNzqnMX7HNdI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "rgbd_dataset = glob.glob('rgbd/image/*.jpg') # TODO fix it\n",
    "image_number = 3 # [0,4] there are five different images in the folder\n",
    "\n",
    "image_file = rgbd_dataset[image_number]\n",
    "depth_file = image_file.replace('image','depth')[:-4] + '.png'\n",
    "assert os.path.isfile(image_file); \n",
    "assert os.path.isfile(depth_file);\n",
    "color_image, depth_image = load_rgbd_image(image_file, depth_file)\n",
    "plot_rgbd(color_image, depth_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GPrqYCQHNdN"
   },
   "outputs": [],
   "source": [
    "# Next we can convert the depth image into a point cloud \n",
    "def show_point_cloud(color_raw, depth_raw):\n",
    "    rgbd_image = o3d.geometry.RGBDImage.create_from_sun_format(color_raw, depth_raw);\n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, \n",
    "                 o3d.camera.PinholeCameraIntrinsic(o3d.camera.PinholeCameraIntrinsicParameters.PrimeSenseDefault))\n",
    "    # Flip it, otherwise the pointcloud will be upside down\n",
    "    pcd.transform([[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mn3oPBe8HNdS"
   },
   "outputs": [],
   "source": [
    "pcd = show_point_cloud(color_image, depth_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkEWvOGKHNdW"
   },
   "outputs": [],
   "source": [
    "# Lets have a look at the structure of the point cloud data\n",
    "pcd_points = np.asarray(pcd.points)\n",
    "print(\"Point cloud data - shape:\", pcd_points.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BADBLF0VHNdX"
   },
   "source": [
    "Point cloud data is represented as an unsorted set of the size M x N., where M is the number of points and N is the x,y,z value for each point. If you are interested you can access the color data in `pcd.colors`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-PI0jUiHNdY"
   },
   "source": [
    "Let us try to perform a segmentation on the image we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CSsyGtEHNdY"
   },
   "outputs": [],
   "source": [
    "# Setting the number of clusters\n",
    "K = 5\n",
    "# Note: it's just a simple train model run\n",
    "# To improve it you can adapt the best_segment() \n",
    "# to generate the clusters with the best model\n",
    "initial_params = initialize_parameters(pcd_points, K)\n",
    "MU, SIGMA, PI, r = train_model(pcd_points, K,\n",
    "                               convergence_function=default_convergence,\n",
    "                               initial_values=initial_params)\n",
    "clusters = cluster(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWROpiDyHNda"
   },
   "outputs": [],
   "source": [
    "# Generate a set of size K of distinct color to plot the clusters\n",
    "# Adapted from https://stackoverflow.com/questions/876853/generating-color-ranges-in-python\n",
    "import colorsys\n",
    "HSV_tuples = [(x*1.0/K, 1.0, 1.0) for x in range(K)]\n",
    "color_maps = list(map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOBHN7n8HNdd"
   },
   "source": [
    "### Visualizing the segmented point cloud data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_El0ZyvHNdf"
   },
   "outputs": [],
   "source": [
    "or_pcd = o3d.geometry.PointCloud() # Create new point cloud handler\n",
    "or_pcd.points = o3d.utility.Vector3dVector(pcd_points) # set point cloud data\n",
    "colors = np.zeros_like(pcd_points) # initialize colors to 0\n",
    "for i, point in enumerate(np.unique(clusters)):\n",
    "    random_color = color_maps[i]\n",
    "    cluster_mask = (clusters == point) # get the mask of the cluster i\n",
    "    colors[cluster_mask,:] = random_color # set random color to all the point of this segment\n",
    "or_pcd.colors = o3d.utility.Vector3dVector(colors) # set color data\n",
    "o3d.visualization.draw_geometries([or_pcd]) # visualize point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZXEC7QnHNdj"
   },
   "source": [
    "Some questions to think about:\n",
    "- Would adding a color help or harm the segmentation results?\n",
    "- How about the case: segment RGB data -> add depth -> convert to Point Cloud -> cluster? Would that help/harm?\n",
    "- Could you think of a way you could compress the point cloud data?\n",
    "\n",
    "Things to try:\n",
    "- Segmentation here is done in purely unsupervised manner, you could manually combine multiple gaussian\n",
    "- How about merging multiple scenes into a single one? You could crop one segment from one scene and place it inside another scene.\n",
    "- Try K-means on point cloud data and see what results does it produces\n",
    "- Can we omit the step of conversion to point cloud? And use depth only? Or depth with x,y coordinates?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
